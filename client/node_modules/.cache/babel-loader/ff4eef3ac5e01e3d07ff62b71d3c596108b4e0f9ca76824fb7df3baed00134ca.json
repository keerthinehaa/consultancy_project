{"ast":null,"code":"// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../../resource.mjs\";\nimport * as Core from \"../../core.mjs\";\nexport class Transcriptions extends APIResource {\n  /**\r\n   * Transcribes audio into the input language.\r\n   */\n  create(body, options) {\n    return this._client.post('/openai/v1/audio/transcriptions', Core.multipartFormRequestOptions({\n      body,\n      ...options\n    }));\n  }\n}","map":{"version":3,"names":["APIResource","Core","Transcriptions","create","body","options","_client","post","multipartFormRequestOptions"],"sources":["D:\\consultancy_project\\consultancy_project\\client\\node_modules\\groq-sdk\\src\\resources\\audio\\transcriptions.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\r\n\r\nimport { APIResource } from '../../resource';\r\nimport * as Core from '../../core';\r\n\r\nexport class Transcriptions extends APIResource {\r\n  /**\r\n   * Transcribes audio into the input language.\r\n   */\r\n  create(body: TranscriptionCreateParams, options?: Core.RequestOptions): Core.APIPromise<Transcription> {\r\n    return this._client.post(\r\n      '/openai/v1/audio/transcriptions',\r\n      Core.multipartFormRequestOptions({ body, ...options }),\r\n    );\r\n  }\r\n}\r\n\r\n/**\r\n * Represents a transcription response returned by model, based on the provided\r\n * input.\r\n */\r\nexport interface Transcription {\r\n  /**\r\n   * The transcribed text.\r\n   */\r\n  text: string;\r\n}\r\n\r\nexport interface TranscriptionCreateParams {\r\n  /**\r\n   * ID of the model to use. Only `whisper-large-v3` is currently available.\r\n   */\r\n  model: (string & {}) | 'whisper-large-v3';\r\n\r\n  /**\r\n   * The audio file object (not file name) to transcribe, in one of these formats:\r\n   * flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. Either a file or a URL must\r\n   * be provided. Note that the file field is not supported in Batch API requests.\r\n   */\r\n  file?: Core.Uploadable;\r\n\r\n  /**\r\n   * The language of the input audio. Supplying the input language in\r\n   * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will\r\n   * improve accuracy and latency.\r\n   */\r\n  language?:\r\n    | (string & {})\r\n    | 'en'\r\n    | 'zh'\r\n    | 'de'\r\n    | 'es'\r\n    | 'ru'\r\n    | 'ko'\r\n    | 'fr'\r\n    | 'ja'\r\n    | 'pt'\r\n    | 'tr'\r\n    | 'pl'\r\n    | 'ca'\r\n    | 'nl'\r\n    | 'ar'\r\n    | 'sv'\r\n    | 'it'\r\n    | 'id'\r\n    | 'hi'\r\n    | 'fi'\r\n    | 'vi'\r\n    | 'he'\r\n    | 'uk'\r\n    | 'el'\r\n    | 'ms'\r\n    | 'cs'\r\n    | 'ro'\r\n    | 'da'\r\n    | 'hu'\r\n    | 'ta'\r\n    | 'no'\r\n    | 'th'\r\n    | 'ur'\r\n    | 'hr'\r\n    | 'bg'\r\n    | 'lt'\r\n    | 'la'\r\n    | 'mi'\r\n    | 'ml'\r\n    | 'cy'\r\n    | 'sk'\r\n    | 'te'\r\n    | 'fa'\r\n    | 'lv'\r\n    | 'bn'\r\n    | 'sr'\r\n    | 'az'\r\n    | 'sl'\r\n    | 'kn'\r\n    | 'et'\r\n    | 'mk'\r\n    | 'br'\r\n    | 'eu'\r\n    | 'is'\r\n    | 'hy'\r\n    | 'ne'\r\n    | 'mn'\r\n    | 'bs'\r\n    | 'kk'\r\n    | 'sq'\r\n    | 'sw'\r\n    | 'gl'\r\n    | 'mr'\r\n    | 'pa'\r\n    | 'si'\r\n    | 'km'\r\n    | 'sn'\r\n    | 'yo'\r\n    | 'so'\r\n    | 'af'\r\n    | 'oc'\r\n    | 'ka'\r\n    | 'be'\r\n    | 'tg'\r\n    | 'sd'\r\n    | 'gu'\r\n    | 'am'\r\n    | 'yi'\r\n    | 'lo'\r\n    | 'uz'\r\n    | 'fo'\r\n    | 'ht'\r\n    | 'ps'\r\n    | 'tk'\r\n    | 'nn'\r\n    | 'mt'\r\n    | 'sa'\r\n    | 'lb'\r\n    | 'my'\r\n    | 'bo'\r\n    | 'tl'\r\n    | 'mg'\r\n    | 'as'\r\n    | 'tt'\r\n    | 'haw'\r\n    | 'ln'\r\n    | 'ha'\r\n    | 'ba'\r\n    | 'jv'\r\n    | 'su'\r\n    | 'yue';\r\n\r\n  /**\r\n   * An optional text to guide the model's style or continue a previous audio\r\n   * segment. The [prompt](/docs/speech-text) should match the audio language.\r\n   */\r\n  prompt?: string;\r\n\r\n  /**\r\n   * The format of the transcript output, in one of these options: `json`, `text`, or\r\n   * `verbose_json`.\r\n   */\r\n  response_format?: 'json' | 'text' | 'verbose_json';\r\n\r\n  /**\r\n   * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the\r\n   * output more random, while lower values like 0.2 will make it more focused and\r\n   * deterministic. If set to 0, the model will use\r\n   * [log probability](https://en.wikipedia.org/wiki/Log_probability) to\r\n   * automatically increase the temperature until certain thresholds are hit.\r\n   */\r\n  temperature?: number;\r\n\r\n  /**\r\n   * The timestamp granularities to populate for this transcription.\r\n   * `response_format` must be set `verbose_json` to use timestamp granularities.\r\n   * Either or both of these options are supported: `word`, or `segment`. Note: There\r\n   * is no additional latency for segment timestamps, but generating word timestamps\r\n   * incurs additional latency.\r\n   */\r\n  timestamp_granularities?: Array<'word' | 'segment'>;\r\n\r\n  /**\r\n   * The audio URL to translate/transcribe (supports Base64URL). Either a file or a\r\n   * URL must be provided. For Batch API requests, the URL field is required since\r\n   * the file field is not supported.\r\n   */\r\n  url?: string;\r\n}\r\n\r\nexport declare namespace Transcriptions {\r\n  export { type Transcription as Transcription, type TranscriptionCreateParams as TranscriptionCreateParams };\r\n}\r\n"],"mappings":"AAAA;SAESA,WAAW,QAAE;OACf,KAAKC,IAAI;AAEhB,OAAM,MAAOC,cAAe,SAAQF,WAAW;EAC7C;;;EAGAG,MAAMA,CAACC,IAA+B,EAAEC,OAA6B;IACnE,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CACtB,iCAAiC,EACjCN,IAAI,CAACO,2BAA2B,CAAC;MAAEJ,IAAI;MAAE,GAAGC;IAAO,CAAE,CAAC,CACvD;EACH","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}